<div otvl-web>
type: sf-img
src: /assets/images/k3s-loc-sp/versEtangDeSoulcem.jpg
alt: Article image
title: Vers l'étang de Soulcem
class_: v-img-header
</div>

# Developing and simple hosting with K3s

<div otvl-web>
type: sf-page-dates
</div>

## Introduction

[K3s](https://k3s.io/) is a lightweight yet very effective
[Kubernetes](https://kubernetes.io/) open source distribution.

While Docker and the Kubernetes orchestrator are leveraged more and more often
for hosting application components,
two interesting questions often remain open:

- how to work locally as a developer:
this is useful to set up, test and debug
the deployment more efficiently than if working remotely,
but also to leverage local computing resources
- how to orchestrate application containers in production environments
where there is no managed Kubernetes solution, or no cost-effective one either

This article provides feedback for deploying and using K3s for development
and simple production.

## Installing K3s for development

### Kubernetes control plane installation

Depending on your way of working and preference,
you can install K3s either directly on the development host if it is running Linux,
or on a separate Linux Virtual Machine that you will access remotely with `kubectl`.
Unless you want to test multi-nodes specific use-cases in development,
the default installation with all the components on a single node
is all what you need.

K3s installs the container runtime `[containerd](https://github.com/containerd/containerd)`
for running Docker-compatible containers, and that is a strong system requirement.
If you are already running Docker on the host, install K3s on a separate Virtual Machine,
or configure the installer to
[leverage](https://docs.k3s.io/advanced#using-docker-as-the-container-runtime)
it.

When ready, connect to the target host and run:

    :::text
    # curl -sfL https://get.k3s.io | sh -

Depending on system and network resources,
this single command will take a few dozens of seconds and deploy a fully operational
Kubernetes cluster as detailed below.

It is worth mentioning that the resulting cluster
will restart happily after a K3s host restart,
making the solution convenient when the development environment is stopped.

That being said,
this of course doesn't prevent application pods to take care of their data by themselves.

### Analyzing the K3s architecture

Having a look at the [architecture document](https://docs.k3s.io/architecture),
it means that we run the K3s server components on the installed host,
and there is no additional K3s agent node.

We find on this document the following familiar Kubernetes components:

- "`api-server`" is the cluster API entry point;
- "`scheduler`" is responsible to allocate resources to the containers to be run;
- "`controller-manager`" is responsible to apply requested changes on the cluster;
- "`kubelet`" is an agent running on each node to control the state of containers;
- "`kube-proxy`" is responsible to configure network rules to enable secured network
  communication among the components and with external systems;

We also find the "`kine`" component that,
in this default installation,
enables the K3s server to store the cluster configuration and state
in a `SQLite` database, but could leverage `etcd` as well.

<div otvl-web>
type: sf-img
src: /assets/images/k3s-loc-sp/architecture.png
alt: Deployed architecture schema
title: K3s deployed architecture
class_: v-img
</div>

What is specific to K3s is that those components are packaged and run
in a single Linux process,
making the most efficient use of CPU, memory and network resources.
Other Kubernetes distributions
typically run them as individual containers and Linux system services.

    :::text
    # systemctl status k3s.service
    ● k3s.service - Lightweight Kubernetes
         Loaded: loaded (/etc/systemd/system/k3s.service; enabled; preset: enabled)
         Active: active (running) since Wed 2024-01-03 09:21:54 CET; 3h 10min ago
           Docs: https://k3s.io
       Main PID: 2570 (k3s-server)
          Tasks: 213
         Memory: 1.7G
            CPU: 26min 24.955s

A few containers are also deployed for the control plane:

    :::text
    # kubectl get pods -n kube-system
    NAME                                      READY   STATUS      RESTARTS   AGE
    local-path-provisioner-84db5d44d9-72qfw   1/1     Running     0          4h50m
    helm-install-traefik-crd-w7fs6            0/1     Completed   0          4h50m
    coredns-6799fbcd5-bctpp                   1/1     Running     0          4h50m
    helm-install-traefik-6lmt2                0/1     Completed   1          4h50m
    svclb-traefik-e23bb5a4-4zrcl              2/2     Running     0          4h47m
    traefik-8c645c69c-xrrcr                   1/1     Running     0          4h47m
    metrics-server-67c658944b-rlrzk           1/1     Running     0          4h50m

Among those, [Traefik](https://doc.traefik.io/traefik/)
is a reverse proxy and load balancer
that in our case is also able to play the role of a Kubernetes
[Ingress Controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/).
Traefik can use an ACME provider such as Let's Encrypt to request and renew TLS certificates
corresponding to the DNS of the hosted applications services
as declared in Kubernetes
[Ingresses](https://kubernetes.io/docs/concepts/services-networking/ingress/).

## Switching from Docker to Containerd

If you chose to use the default container runtime installation of K3s,
things are a little bit less easy then with using the perfectly packaged Docker product.

Here are some installation steps to be performed on the K3s server.

### Meet Nerdcrl

First thing is to install a user-friendly command line tool to work with containers.
This tool is open source too and is called [`Nerdctl`](https://github.com/containerd/nerdctl).
Is it for nerds? Sure, but not only. It is firstly to control contai-nerd.

You can find uses of `ctr` and `crictl` tools too,
and they are installed with K3s,
but they are meant
for low-level interactions with containers as explained
[here](https://github.com/containerd/nerdctl/blob/main/docs/faq.md#how-is-nerdctl-different-from-ctr-and-crictl-).

A minimal installation is enough in the case K3s is already installed.
The rootless mode is not necessary either when you don't have specific security constraints.
So just download the
[binary](https://github.com/containerd/nerdctl/releases)
and install it in the PATH environment
for instance `/usr/local/bin`.

Nerdctl must be configured to use the K3s containerd installation:

    :::text
    # vi /etc/nerdctl/nerdctl.toml
    address        = "/run/k3s/containerd/containerd.sock"
    namespace      = "k8s.io"

An ansible role for doing that is available
[here](https://github.com/t-beigbeder/otvl_ansible/tree/master/src/ansible/playbooks/otvl_service/otvl_nerdctl).

Nerdctl provides a
[CLI](https://github.com/containerd/nerdctl/blob/main/docs/command-reference.md)
mostly compatible with Docker's one, just change `docker` with `nerdctl`,
which makes the switch as smooth as possible.
You can control images and containers as usual.
You can even launch docker compose stacks.
But you cannot build new images yet.

### Meet BuildKit

Nerdctl is able to build docker images with the familiar `nerdctl build` sub-command
as soon as [BuildKit](https://github.com/moby/buildkit) is installed.

Download BuildKit binaries from the project
[releases](https://github.com/moby/buildkit/releases)
and install them in the PATH environment
for instance `/usr/local/bin`.

BuildKit must be configured to use the K3s containerd installation:

    :::text
    # vi /etc/buildkit/buildkitd.toml
    [worker.oci]
      enabled = false

    [worker.containerd]
      enabled = true
      address = "/run/k3s/containerd/containerd.sock"
      namespace = "k8s.io"

The buildkitd daemon must be installed as a systemd service:

    :::text
    # vi /etc/systemd/system/buildkit.service
    [Unit]
    Description=BuildKit
    Documentation=https://github.com/moby/buildkit

    [Service]
    Type=simple
    TimeoutStartSec=10
    Restart=always
    RestartSec=10
    ExecStart=/usr/local/bin/buildkitd

    [Install]
    WantedBy=multi-user.target

    # systemctl daemon-reload
    # systemctl enable buildkit.service
    # systemctl start buildkit.service

An ansible role for doing that is available
[here](https://github.com/t-beigbeder/otvl_ansible/tree/master/src/ansible/playbooks/otvl_service/otvl_buildkit).

Now you are ready to use the `nerdctl build` subcommand for building new Docker images.

The images are available locally for direct use by Kubernetes Pod containers.

### Kubectl configuration

To access the K3s cluster with `kubectl` from a remote or non-root position
just copy the file `/etc/rancher/k3s/k3s.yaml`
into the local file `$HOME/.kube/config`,
then change the server url

    :::text
    server: https://127.0.0.1:6443

with the name of the K3s server

    :::text
    server: https://<k3s-server-name>:6443

### Ready for development

The Kubernetes environment is now fully operational for the development activities:

- building container images
- deploying Kubernetes workloads and related resources
- accessing hosted workloads through the Ingress Controller and other gateway services

As K3s is production-ready, could the previous kind of deployment be used
for hosting production workloads?

## Simple hosting for production

### Introduction

Many deployment scenarios require a high level of availability.
While K3s provides all the required features for
[supporting](https://docs.k3s.io/architecture#high-availability-k3s)
it,
this is obviously not the case of this simple deployment,
as there are those two main single points of failure:

- the server node for the K3S server and user-plane workloads
- the SQLite database for storing the cluster configuration and state

Anyway, high availability does not necessarily mean _always available_,
and even this simple deployment can provide a very good availability level
as soon as it is effectively backed with automatic deployment.
And as the deployment is lightweight like we saw it, the recovery time can be pretty low.

We could say that the lack of redundancy is compensated
with the simplicity of what is to be deployed.

### TBC registry

How to host a registry.

## References

- [K3s project](https://github.com/k3s-io/k3s/)
- [K3s documentation](https://docs.k3s.io/)
- [Containerd project](https://github.com/containerd/containerd)
- [Traefik documentation](https://doc.traefik.io/traefik/)
- [Nerdctl](https://github.com/containerd/nerdctl)
- [BuildKit](https://github.com/moby/buildkit)
